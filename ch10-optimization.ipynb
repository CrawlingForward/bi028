{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Python Optimization\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course plan\n",
    "\n",
    "**Introduction**\n",
    "    1. Basics: what is an optimization problem, how to solve, and line search\n",
    "**Unconstrained optimization**\n",
    "    2. Direct search methods: Hooke&Jeeves and Powell's methods \n",
    "    3. Steepest Descent and Newton's method for unconstrained optimization\n",
    "    4. Example of using available software: `scipy.optimize`\n",
    "**Constrained optimization**\n",
    "    5. Indirect methods for constrained optimization\n",
    "    6. Direct methods for constrained optimization\n",
    "    7. Optimality conditions\n",
    "    8. Methods using optimality conditions for constrained optimization\n",
    "    9. Algebraic modeling languages, especially Pyomo\n",
    "**Multiobjective optimization**\n",
    "    10. What is multiobjective optimization\n",
    "    11. How to solve multiobjective optimization problems\n",
    "**Applications of optimization**\n",
    "    12. How to find and read scientific papers in the field\n",
    "    13. Students finding applications of optimization themselves\n",
    "**Wrapping up**\n",
    "    14. Further topics and current research in optimization\n",
    "    15. Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Optimization Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 What is an optimization problem?\n",
    "\n",
    "A general mathematical formulation for **the optimization problems** is\n",
    "$$\n",
    "\\begin{align} \\\n",
    "\\min \\quad &f(x)\\\\\n",
    "\\textit{s.t.} \\quad & g_j(x) \\le 0\\text{ for all }j=1,\\ldots,J\\\\\n",
    "& h_k(x) = 0\\text{ for all }k=1,\\ldots,K\\\\\n",
    "&x\\in \\mathbb R^n.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The above problem can be expressed as \n",
    ">Find an $x\\in \\mathbb R^n$ such that $g_j(x)\\le 0$ for all $j=1,\\ldots,J$ and $h_k(x)=0$ for all $k=1,\\ldots,K$, and there does not exist $x'\\in \\mathbb R^n$ satisfying $f(x')<f(x)$ and $g_j(x')\\geq 0$ for all $j=1,\\ldots,J$, $h_k(x')=0$ for all $k=1,\\ldots,K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Some important concepts\n",
    "\n",
    "There are three main components to an optimization problem:\n",
    "* the variables $x$ are called the **decision variables (决策变量)**,\n",
    "* the equalities and inequalities $g_j(x)\\geq 0$ and $h_k(x)=0$ are called the **constraints (约束)**,\n",
    "* the funtion $f(x)$ is called the **objective function (目标函数)**.\n",
    "\n",
    "Values of decision variables $x^*$ are called **solutions(解)** and a solution is called\n",
    "* **feasible (可行解)** if $g_j(x^*)\\geq 0$ for all $j=1,\\ldots,J$, $h_k(x^*)=0$ for all $k=1,\\ldots,K$,\n",
    "* **locally optimal (局部最优解)** if $x^*$ is feasible and there exists $r>0$ such that there does not exist a feasible solution $x'\\in \\operatorname{B}(x^*,r)$ such that $f(x')<f(x^*)$, and\n",
    "* **optimal (最优解)** if $x^*$ is feasible and there does not exist a feasible solution $x'$ such that $f(x')<f(x^*)$.\n",
    "\n",
    "The problem is called\n",
    "* **linear/nonlinear (线性/非线性)** if the objective function and the constraints of the problem are/are not affinely linear,\n",
    "* **multi/unimodal (多模/单模)** if the problem has/does not have more than one local optimum,\n",
    "* **convex/nonconvex (凸/非凸)** if the objective and the constraints are that,\n",
    "* **continuous/differentiable/twice-differentiable, etc (连续/可微/二次可微)** if the objective and the constraints are that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How to solve optimization problems?\n",
    "\n",
    "### 2.1 Iterative vs. non-iterative methods\n",
    "\n",
    "Optimal solutions to some optimization problems can be found by defining an explicit formula for it. For example, if *the objective function is twice continuously differentiable （二阶连续可导）and there are no constraints （无约束）*, the optimal solution (if exists) can be found by calculating all the zero-points of the gradient and finding the best one of those. In this kind of cases, the optimization problem can be solved using **non-iterative methods.**\n",
    "\n",
    "*When the problem has constraints, or the problem is in some other way not-well behaved*, ** iterative methods** are needed. In iterative methods, the solving the optimizaiton problem starts from a **initial** solution and then tries to *improve the solution iteratively*. The optimization algorithm chooses how the solution is changed at each iteration.\n",
    "\n",
    "Often the methods cannot guarantee a (global) optimum, but instead **we need to satisfy ourselves with a local optimum**. In addition, it is usually not possible to find the actual optimal solution, but instead **an approximation of the optimal solution**. A feasible solution $x^*$ is called an approximation of a local optimum $x^{**}$ with quality $r>0$, when $\\|x^*-x^{**}\\|\\leq r$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Line search (线性搜索)\n",
    "\n",
    "For a box-constrained optimization problem\n",
    "$$\n",
    "\\min_x\\;f(x)\\quad \\textit{s.t.}\\;x \\in [a,b]\n",
    "$$\n",
    "where $a,b\\in\\mathbb R$.\n",
    "\n",
    "Let's illustrate how to find an approximation of a local optimum using an example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\" An example optimization problem. \"\"\"\n",
    "    return 2+(1-x)**2\n",
    "\n",
    "print \"The value of the objective function at 0 is\", f(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\S$ Example: Line search with fixed steps\n",
    "**Input**: $r>0$.  \n",
    "**Output**: $x^*$.<br>\n",
    "**LineSearch**: \n",
    "1. Initiate $x \\in [a, b]$\n",
    "2. Loop:\n",
    "```\n",
    "    If f(x+r) > f(x):\n",
    "        stop, and the locally optimal solution is x \n",
    "    x = x + r\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fixed_linesearch(a,b,f,r):\n",
    "    x = a\n",
    "    while f(x)>f(x+r) and x+r<b:\n",
    "        x=x+r\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = fixed_linesearch(0.0, 3.0, f, 1e-3)\n",
    "print \"Line search finds that the optimal point in [0.0, 3.0] is\", x, \", and the optimum is\", f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit fixed_linesearch(0.0, 3.0, f, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Bisection search (二分线性搜索)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input:** $r>0$ and $\\epsilon > 0$.  \n",
    "**Output:** $x^*$.<br/>\n",
    "**Bisection Search**:\n",
    "```\n",
    "Set x=a and y=b\n",
    "while y-x > 2*r:\n",
    "    if f((x+y)/2+eps) > f((x+y)/2-eps):\n",
    "        set y = (x+y)/2\n",
    "    otherwise:\n",
    "        set x = (x+y)/2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bisection(a,b,f,L,epsilon):\n",
    "    x = a\n",
    "    y = b\n",
    "    while y-x>2*L:\n",
    "        if f((x+y)/2+epsilon)>f((x+y)/2-epsilon):\n",
    "            y=(x+y)/2+epsilon\n",
    "        else:\n",
    "            x = (x+y)/2-epsilon\n",
    "    return (x+y)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = bisection(0.0, 3.0, f, 1e-3, 1e-4)\n",
    "print \"Bisection line search finds that the optimal point in [0.0, 3.0] is\", x, \", and the optimum is\", f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit bisection(0.0, 3.0, f, 1e-3, 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Golden-section line search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Golden section\n",
    "\n",
    "Let $a<c<b$ be such that $\\frac{b-a}{c-a}=\\frac{c-a}{b-c}$. Then it is said that the point $c$ devides interval $[a,b]$ in the ratio of golden section (from the left, mirror from the right). Note that $c=a+\\frac{\\sqrt{5}-1}2(b-a)\\approx a+0.618(b-a)$.\n",
    "\n",
    "There is a theorem that if $a<c<d<b$ and both points divide the interval $[a,b]$ in the ratio of golden section (from right and left), then the point $c$ divides the interval $[a,d]$ in the ratio of golder ration from the left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input:** the quality $r>0$ of the approximation of the local optimum.  \n",
    "**Output:** an approximation of the local optimum with quality $r$.<br/>\n",
    "**GoldenSection:**\n",
    "```\n",
    "Set x=a and y=b\n",
    "while y-x>2*r:\n",
    "    Get left and right golden division point c < d for the interval [x,y]\n",
    "    If f(d) > f(c): \n",
    "        set y=d\n",
    "    otherwise:\n",
    "        set x=c\n",
    "return (x+y)/2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def golden_section(a, b, f, r):\n",
    "    x, y = a, b\n",
    "    while y - x > 2*r:\n",
    "        c = y - (math.sqrt(5)-1)*(y-x)/2\n",
    "        d = x + (math.sqrt(5)-1)*(y-x)/2\n",
    "        if f(d) > f(c):\n",
    "            y = d\n",
    "        else:\n",
    "            x = c\n",
    "    return (x+y)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = golden_section(0.0, 3.0, f, 1e-3)\n",
    "print \"Golden section line search finds that the optimal point in [0.0, 3.0] is\", x, \", and the optimum is\", f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit golden_section(0.0, 3.0, f, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Direct search: Hooke&Jeeves and Powell's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start studying functions of multiple variables by studying unconstrained optimization problems\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\quad &f(x)\\\\\n",
    "\\text{s.t.}\\quad &x\\in \\mathbb R^n\n",
    "\\end{align}  \n",
    "$$\n",
    "\n",
    "And here is an example:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\quad & (x_1-10)^2+(x_2+5)^2+x_1^2\\\\\n",
    "\\text{s.t.}\\quad &x_1,x_2\\in\\mathbb R\n",
    "\\end{align}  \n",
    "$$\n",
    "This problem is unconstrained, because there are no constraints.\n",
    "\n",
    "Now we need to redefine a function in Python, which is a two-variable function \n",
    "$$f:(x_1,x_2)\\to (x_1-10)^2+(x_2+5)^2+x_1^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f_simple(x):\n",
    "    return (x[0] - 10.0)**2 + (x[1] + 5.0)**2+x[0]**2\n",
    "print \"At point (3,-8) the value of the function is\",f_simple([3,-8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also plot the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pylab import meshgrid\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_2d_function(lb1,lb2,ub1,ub2,f):\n",
    "    x = np.arange(lb1,ub1,0.1)\n",
    "    y = np.arange(lb2,ub2,0.1)\n",
    "    X,Y = meshgrid(x, y) # grid of point\n",
    "    Z = [f([x,y]) for (x,y) in zip (X,Y)] # evaluation of the function on the grid\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    surf = ax.plot_surface(X, Y, Z)\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_2d_function(3,-7,7,-3,f_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Direct search methods\n",
    "\n",
    "**Direct search methods** (also called pattern search methods) rely only on the function values to find a (local) optimum. Direct search methods consist of a set of  \n",
    "1. **Exploratory moves** that acquire information about the function $f$ in the neighbourhood of current solution, and\n",
    "2. **Pattern moves** that attempt to speed up the search using the information acquired in the exploratory moves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Hooke&Jeeves algorithm\n",
    "\n",
    "**input:** a minimum step length $L>0$, initial step length $\\epsilon_0$, constant $0<\\delta<1$ for reducing the step length, exploratory step multiplier $\\gamma>1$, and starting solution $x_0$<br/>\n",
    "**output:** an approximation of a local optimum (no guarantees of quality in general cases)  \n",
    "```\n",
    "set eps=eps0\n",
    "set x=x0\n",
    "while eps > L:\n",
    "    for each coordinate direction i:\n",
    "        find the smallest of function values by incrementing and reducing the variable value in that coordinate by eps, let this value be xi*\n",
    "    if x==(x1*,...,xn*):\n",
    "        reduce eps = delta*eps\n",
    "    else:\n",
    "        if f(x1*,...,xn*) < f(x+gamma*((x1*,...,xn*)-x)):\n",
    "            set x = (x1*,...xn*)\n",
    "        else:\n",
    "            set x = x+gamma*((x1*,...,xn*)-x)\n",
    "return x\n",
    "        \n",
    "```\n",
    "Thus, \n",
    "* the **exploratory step** of Hooke&Jeeves is performed by incrementing and reducing the variable to each coordinate direction and \n",
    "* the **pattern move** is just a multiplication of the exploratory move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy #Copying vectors\n",
    "import numpy as np #Import vector calculus and much more!\n",
    "def hookejeeves(L,epsilon0,delta,gamma,x0,f):\n",
    "    #Set up the initial values\n",
    "    epsilon = epsilon0\n",
    "    x = np.array(x0)\n",
    "    #Loop while step length greater than L:\n",
    "    while epsilon>L:\n",
    "        #our exploratory move is initially [0,..,0]\n",
    "        xtest = np.zeros(len(x))\n",
    "        for coordinate in range(len(x)):\n",
    "            #First points to be explored are the all x, to be changed\n",
    "            exp_points = [copy.copy(x) for _ in range(3)] #points to be explored\n",
    "            #Change exp_points[0] and exp_points[1] to reflect\n",
    "            #moving along the coordinate\n",
    "            exp_points[0][coordinate]-=epsilon\n",
    "            exp_points[1][coordinate]+=epsilon\n",
    "            #Assign the function values given by exp_points to a list\n",
    "            f_exp_points = [f(exp_point) for exp_point in exp_points]\n",
    "            #pick the smallest one of them\n",
    "            min_value = min(f_exp_points)\n",
    "            #The exploratory move to the coordinate direction is given by the\n",
    "            #move giving the smallest value of f\n",
    "            xtest[coordinate] = exp_points[f_exp_points.index(min_value)][coordinate] #The coordinate value is the one where the minimum is attained\n",
    "        #If no move at all, then reduce the exploratory move step size\n",
    "        if all(xtest==x):\n",
    "            epsilon = delta*epsilon\n",
    "        else:\n",
    "            #if exploratory move is better than pattern move\n",
    "            if f(xtest)<f(x+gamma*(xtest-x)):\n",
    "                #...set x as the exploratory move\n",
    "                x = xtest\n",
    "            else:\n",
    "                #Otherwise we take the pattern move\n",
    "                x = x+gamma*(xtest-x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L = 0.001\n",
    "epsilon0 = 1\n",
    "delta = 0.1\n",
    "gamma = 2.0\n",
    "start = [0.0,0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = hookejeeves(L,epsilon0,delta,gamma,start,f_simple)\n",
    "print \"Optimal solution is\", x, \"and the optimal objective value is\", f_simple(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit hookejeeves(L,epsilon0,delta,gamma,start,f_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the search process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_2d_steps(steps,start):\n",
    "    myvec = np.array([start]+steps).transpose()\n",
    "    plt.plot(myvec[0,],myvec[1,],'ro')\n",
    "    for label,x,y in zip([str(i) for i in range(len(steps)+1)],myvec[0,],myvec[1,]):\n",
    "        plt.annotate(label,xy = (x+.2, y))\n",
    "    plt.xlim(min(myvec[0])-1, max(myvec[0])+1)\n",
    "    plt.ylim(min(myvec[1])-1, max(myvec[1])+1)\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy #Copying vectors\n",
    "import numpy as np #Import vector calculus and much more!\n",
    "def hookejeeves_savesteps(L,epsilon0,delta,gamma,x0,f):\n",
    "    epsilon = epsilon0\n",
    "    x = np.array(x0)\n",
    "    steps = []\n",
    "    while epsilon>L:\n",
    "        xtest = np.zeros(len(x))\n",
    "        for coordinate in range(len(x)):\n",
    "            exp_points = [copy.copy(x) for _ in range(3)] #points to be explored\n",
    "            exp_points[0][coordinate]-=epsilon\n",
    "            exp_points[1][coordinate]+=epsilon\n",
    "            f_exp_points = [f(exp_point) for exp_point in exp_points]\n",
    "            min_value = min(f_exp_points)\n",
    "            xtest[coordinate] = exp_points[f_exp_points.index(min_value)][coordinate] \n",
    "            #The coordinate value is the one where the minimum is attained\n",
    "        if all(xtest==x):\n",
    "            epsilon = delta*epsilon\n",
    "        else:\n",
    "            if f(xtest)<f(x+gamma*(xtest-x)):\n",
    "                x = xtest\n",
    "            else:\n",
    "                x = x+gamma*(xtest-x)\n",
    "            steps.append(x)\n",
    "    return x,steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L = 0.001\n",
    "epsilon0 = 1.0\n",
    "delta = 0.1\n",
    "gamma = 0.1\n",
    "start = [0.,0.]\n",
    "(x,steps) = hookejeeves_savesteps(L,epsilon0,delta,gamma,start,f_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min([start[0], map(lambda x: x[0], steps)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max([start[0], map(lambda x: x[0], steps)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_2d_steps(steps,start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Powell's method\n",
    "\n",
    "Powell's method is similar to Hooke&Jeeves, but the first step in exploratory moves is taken to the direction of the last pattern move. This speeds up the convergence in most cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "def powell_savesteps(L,epsilon0,delta,gamma,x0,f):\n",
    "    epsilon = epsilon0\n",
    "    exp_direction = np.array([0,1])\n",
    "    x = np.array(x0)\n",
    "    steps = []\n",
    "    while epsilon>L:\n",
    "        exp_direction=epsilon*exp_direction\n",
    "        #Comparing among exploratory points to firtst exploratory direction:\n",
    "        if f(x+exp_direction)<f(x):\n",
    "            exp_step1=exp_direction\n",
    "        elif f(x-exp_direction)<f(x):\n",
    "            exp_step1=-exp_direction\n",
    "        else:\n",
    "            exp_step1 = np.zeros(2)\n",
    "        #The following only works in 2d!!\n",
    "        exp_direction2 = np.array([exp_direction[1],-exp_direction[0]])\n",
    "        if f(x+exp_direction2)<f(x):\n",
    "            exp_step2=exp_direction2\n",
    "        elif f(x-exp_direction2)<f(x):\n",
    "            exp_step2=-exp_direction2\n",
    "        else:\n",
    "            exp_step2 = np.zeros(2)\n",
    "        if all(exp_step1+exp_step2==0):\n",
    "            epsilon = delta*epsilon\n",
    "        else:\n",
    "            if f(x+(exp_step1+exp_step2))<f(x+gamma*(exp_step1+exp_step2)):\n",
    "                x = x+(exp_step1+exp_step2)\n",
    "            else:\n",
    "                x = x+gamma*(exp_step1+exp_step2)\n",
    "            steps.append(x)\n",
    "            exp_direction = (exp_step1+exp_step2)/np.linalg.norm(exp_step1+exp_step2)\n",
    "    return x,steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L = 0.001\n",
    "epsilon0 = 10\n",
    "delta = 0.01\n",
    "gamma = 2.0\n",
    "start = [-2.,1.]\n",
    "(x,steps) = powell_savesteps(L,epsilon0,delta,gamma,start,f_simple)\n",
    "print \"Optimal solution is\" + str(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_2d_steps(steps,start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Steepest descent and Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before that, we need to import a python automated differentiation package `ad`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grad_f, hess_f = ad.gh(f_simple)\n",
    "print \"At the point (1,2) gradient is\", grad_f([1,2]), \"and hessian is\", hess_f([1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function to view the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from pylab import meshgrid\n",
    "def visualize_gradient(f,point,x_lim,y_lim):\n",
    "    grad_point = np.array(ad.gh(f)[0](point))\n",
    "    grad_point = grad_point/np.linalg.norm(grad_point)\n",
    "    X,Y,Z = point[0],point[1],f(point)\n",
    "    U,V,W = grad_point[0],grad_point[1],0\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    x = np.arange(x_lim[0],x_lim[1],0.1)\n",
    "    y = np.arange(y_lim[0],y_lim[1],0.1)\n",
    "    X2,Y2 = meshgrid(x, y) # grid of point\n",
    "    Z2 = [f([x,y]) for (x,y) in zip (X2,Y2)] # evaluation of the function on the grid\n",
    "    surf = ax.plot_surface(X2, Y2, Z2,alpha=0.5)\n",
    "    ax.quiver(X,Y,Z,U,V,W,color='red',linewidth=1.5)\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize_gradient(f_simple,[1,-2],[0,10],[-10,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `lambda` function we can easily visualize gradients of various functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize_gradient(lambda x:3*x[0]+x[1],[1,0],[0,2],[0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Prototype algorithm for the steepest descent and Newton's methods\n",
    "\n",
    "**Input:** function $f$ to be optimized, starting point $x_0$, step length rule $alpha$, stopping rule $stop$<br/>  \n",
    "**Output:** A solution $x^*$ that is close to a locally optimal solution\n",
    "```\n",
    "Initiate x = x0\n",
    "set f_old as a big number and f_new as f(x0)\n",
    "while a stopping criterion has not been met:\n",
    "    f_old = f_new\n",
    "    determine search direction d_h according to the method\n",
    "    determine the step length alpha\n",
    "    set x = x + alpha *d_h\n",
    "    f_new = f(x)\n",
    "return x\n",
    "```\n",
    "\n",
    "The way to determine search direction distinguishes steepest descent algorithm and the Newton algorithm. Different stopping rules and step sizes can be mixed and matched with both algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Steepest descent: for unconstrained problem\n",
    "\n",
    "In the steepest descent algorithm, the search direction is determined by the __negative of the gradient $-\\nabla f(x)$__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ad\n",
    "def steepest_descent(f,start,step,precision):\n",
    "    f_old = float('Inf')\n",
    "    x = np.array(start)\n",
    "    steps = []\n",
    "    f_new = f(x)\n",
    "    while abs(f_old-f_new)>precision:\n",
    "        f_old = f_new\n",
    "        d = -np.array(ad.gh(f)[0](x))\n",
    "        x = x+d*step\n",
    "        f_new = f(x)\n",
    "        steps.append(list(x))\n",
    "    return x,f_new,steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = [2.0,-10.0]\n",
    "(x_value,f_value,steps) = steepest_descent(f_simple,start,0.2,0.0001)\n",
    "print \"Optimal solution is \", x_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the solution path here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_2d_steps(steps,start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Newton's method: Unconstrained problems with existing gradient and hessian\n",
    "\n",
    "In Newton's method, the search direction is determined by both gradient and Hessian of the objective function, as $-H^{-1}\\nabla f(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def newton(f,start,step,precision):\n",
    "    f_old = float('Inf')\n",
    "    x = np.array(start)\n",
    "    steps = []\n",
    "    f_new = f(x)\n",
    "    while abs(f_old-f_new)>precision:\n",
    "        f_old = f_new\n",
    "        H_inv = np.linalg.inv(np.matrix(ad.gh(f)[1](x)))\n",
    "        d = (-H_inv*(np.matrix(ad.gh(f)[0](x)).transpose())).transpose()\n",
    "        #Change the type from np.matrix to np.array so that we can use it in our function\n",
    "        x = np.array(x+d*step)[0]\n",
    "        f_new = f(x)\n",
    "        steps.append(list(x))\n",
    "    return x,f_new,steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = [2.0,-10.0]\n",
    "(x_value,f_value,steps) = newton(f_simple,start,0.5,0.01)\n",
    "print \"Optimal solution is \",x_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_2d_steps(steps,start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Applying `scipy.optimize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Nelder-Mead: Simplex method\n",
    "\n",
    "The Nelder-Mead method does not rely on the gradient and hessian of the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = minimize(f_simple,[0,0],method='Nelder-Mead', \n",
    "         options={'disp': True})\n",
    "print res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result, we can see that:\n",
    "* The optimization process has reached the optimum, and terminated successfully.\n",
    "* The minimum objective value is 50.00\n",
    "* It took 99 iterations to reach the minimum.\n",
    "* The function `f_simple` was evaluated 189 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Conjugate gradient\n",
    "\n",
    "The nonlinear conjugate gradient algorithm was proposed by Polak and Ribiere, which is a variant of the Fletcher-Reeves method. The gradient (`jac`) can either be estimated numerically, or provided by an analytical function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 Estimating the gradient numerically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "res = minimize(f_simple, [0,0], method='CG', #Conjugate gradient method\n",
    "               options={'disp': True})\n",
    "print res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 Specifying the gradient function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ad\n",
    "res = minimize(f_simple, [0,0], method='CG', #Conjugate gradient method\n",
    "               options={'disp': True}, jac=ad.gh(f_simple)[0])\n",
    "print res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Newton's Conjugate gradient method\n",
    "\n",
    "Newton's conjugate gradient method, a.k.a truncated Newton's method. It uses a conjugate gradient method to the compute the search direction. See also TNC method for a box-constrained minimization with a similar algorithm.\n",
    "\n",
    "e Newton-CG algorithm requires both the gradient (`jac`) and the Hessian (`hess, hessp`). The __Jacobian__ should be provided as function calls, while the __Hessian__ can be estimated numerically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = minimize(f_simple, [0,0], method='Newton-CG', #Newton-CG method\n",
    "               options={'disp': True},jac=ad.gh(f_simple)[0])\n",
    "print res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also provide the __Hessian__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = minimize(f_simple, [0,0], method='Newton-CG', #Newton-CG method\n",
    "               options={'disp': True},jac=ad.gh(f_simple)[0],\n",
    "               hess=ad.gh(f_simple)[1])\n",
    "print res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Optimization for function with a single variable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.1 Line search\n",
    "\n",
    "The line search algorithm is as we mentioned previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_singlevar(x):\n",
    "    return 2+(1-x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minimize_scalar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minimize_scalar-brent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minimize_scalar(f_singlevar, bounds=[0.0,3.0], method='Bounded', options={'disp': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.2 Brent method\n",
    "\n",
    "Brent's algorithm is used to find a local minimum. The algorithm uses inverse parabolic interpolation when possible to speed up convergence of the golden section method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minimize_scalar(f_singlevar, bounds=[0,3], method='Brent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.3 Golden section method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minimize_scalar(f_singlevar, bounds=[0,3], method='Golden')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Constrained optimization\n",
    "\n",
    "Now we will turn to study the constrained optimizaton problems i.e., the full problem\n",
    "$$\n",
    "\\begin{align} \\\n",
    "\\min \\quad &f(x)\\\\\n",
    "\\text{s.t.} \\quad & g_j(x) \\geq 0\\text{ for all }j=1,\\ldots,J\\\\\n",
    "& h_k(x) = 0\\text{ for all }k=1,\\ldots,K\\\\\n",
    "&a_i\\leq x_i\\leq b_i\\text{ for all } i=1,\\ldots,n\\\\\n",
    "&x\\in \\mathbb R^n,\n",
    "\\end{align}\n",
    "$$\n",
    "where for all $i=1,\\ldots,n$ it holds that $a_i,b_i\\in \\mathbb R$ or they may also be $-\\infty$ of $\\infty$.\n",
    "\n",
    "For example, we can have an optimization problem\n",
    "$$\n",
    "\\begin{align} \\\n",
    "\\min \\quad &x_1^2+x_2^2\\\\\n",
    "\\text{s.t.} \\quad & x_1+x_2-1\\geq 0\\\\\n",
    "&-1\\leq x_1\\leq 1, x_2\\leq 3.\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The optimization problem can be defined as a Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def f_constrained(x):\n",
    "    return np.linalg.norm(x)**2,[x[0]+x[1]-1],[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(f_val,ieq,eq) = f_constrained([1,0])\n",
    "print \"Value of f is\", f_val\n",
    "if len(ieq)>0:\n",
    "    print \"The values of inequality constraints are:\"\n",
    "    for ieq_j in ieq:\n",
    "        print str(ieq_j)+\", \"\n",
    "if len(eq)>0:\n",
    "    print \"The values of the equality constraints are:\"\n",
    "    for eq_k in eq:\n",
    "        print str(eq_k)+\", \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the feasibility of the solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if all([ieq_j>=0 for ieq_j in ieq]) and all([eq_k==0 for eq_k in eq]):\n",
    "    print \"Solution [1, 0] is feasible\"\n",
    "else:\n",
    "    print \"Solution [1,0] is infeasible\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indirect and direct methods for constrained problems\n",
    "\n",
    "There are two categories of methods for constrained optimization: Indirect and direct methods. The main difference is that\n",
    "1. __Indirect methods__ convert the constrained optimization problem into a single or a sequence of unconstrained optimization problems, that are then solved. Often, the intermediate solutions do not need to be feasible, the sequence of solutions converges to a solution that is optimal (and, thus, feasible).\n",
    "2. __Direct methods__ deal with the constrained optimization problem directly. In this case, the intermediate solutions are feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Indirect approach: Penalty function method\n",
    "\n",
    "The IDAE for __penalty function method__ is to penalize the violations of the constraints by embedding the inequality and equality constraints as the regularization terms into a unconstrained optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let, $\\alpha(x):\\mathbb R^n\\to\\mathbb R$ be a function so that \n",
    "* $\\alpha(x)=0$ for all feasible $x$\n",
    "* $\\alpha(x)>0$ for all infeasible $x$.\n",
    "\n",
    "Define optimization problems\n",
    "$$\n",
    "\\begin{align} \\\n",
    "\\min \\qquad &f(x)+\\lambda\\alpha(x)\\\\\n",
    "\\text{s.t.} \\qquad &x\\in \\mathbb R^n\n",
    "\\end{align}\n",
    "$$\n",
    "for $\\lambda>0$ and $x_p$ be the optimal solutions of these problems.\n",
    "\n",
    "In this case, the optimal solutions $x_p$ converge to the optimal solution of the constrained problem, when $\\lambda \\to\\infty$, if such solution exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The known candidates for penalty functions include\n",
    "* $h_k(x)^2$ for equality constraints,\n",
    "* $\\left(\\min\\{0,g_j(x)\\}\\right)^2$ for inequality constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above penalties, we can build our regularization term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alpha(x,f):\n",
    "    (_,ieq,eq) = f(x)\n",
    "    return sum([min([0,ieq_j])**2 for ieq_j in ieq])+sum([eq_k**2 for eq_k in eq])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and also the penalized function (the final unconstrained objective function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def penalized_function(x,f,lam):\n",
    "    return f(x)[0] + lam*alpha(x,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "penalized_function([-1,0],f_constrained,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this unconstrained optimization problem can now be solved using __Nelder-Mead__ approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = minimize(lambda x:penalized_function(x,f_constrained,10000),\n",
    "         [0,0],method='Nelder-Mead', \n",
    "         options={'disp': True})\n",
    "print res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(f_val,ieq,eq) = f_constrained(res.x)\n",
    "print \"Value of f is \"+str(f_val)\n",
    "if len(ieq)>0:\n",
    "    print \"The values of inequality constraints are:\"\n",
    "    for ieq_j in ieq:\n",
    "        print str(ieq_j)+\", \"\n",
    "if len(eq)>0:\n",
    "    print \"The values of the equality constraints are:\"\n",
    "    for eq_k in eq:\n",
    "        print str(eq_k)+\", \"\n",
    "\n",
    "if all([ieq_j>=0 for ieq_j in ieq]) and all([eq_k==0 for eq_k in eq]):\n",
    "    print \"Solution is feasible\"\n",
    "else:\n",
    "    print \"Solution is infeasible\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But how to set the penalty term $\\lambda$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The penalty term should\n",
    "* be large enough in order for the solutions be close enough to the feasible region, but\n",
    "* not be too large to\n",
    "  * cause numerical problems, or\n",
    "  * cause premature convergence to non-optimal solutions because of relative tolerances.\n",
    "\n",
    "Usually, the penalty term is either\n",
    "* set as big as possible without causing problems (hard to know), or\n",
    "* updated iteratively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Barrier function method: Indirect method\n",
    "\n",
    "The IDEA for barrier function method is to prevent leaving the feasible domain by setting the objective value to be $\\infty$ once outside the feasible region.\n",
    "\n",
    "This method is __only applicable to__ problems with inequality constraints and for which the set \n",
    "$$\\{x\\in \\mathbb R^n: g_j(x)>0\\quad \\forall j=1,\\ldots,J\\}$$\n",
    "is non-empty.\n",
    "\n",
    "Let $\\beta:\\{x \\in \\mathbb R^n: g_j(x)>0\\quad \\forall j=1,\\ldots,J\\}\\to \\mathbb R$ be a function so that $\\beta(x)\\to \\infty$, when $x\\to\\partial\\{x\\in \\mathbb R^n: g_j(x)>0\\quad \\forall j=1,\\ldots,J\\}$, where $\\partial A$ is the boundary of the set $A$. Now, define optimization problem \n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\qquad & f(x) + \\lambda \\beta(x)\\\\\n",
    "\\text{s.t. } \\qquad & x\\in \\{x\\in \\mathbb R^n: g_j(x)>0\\quad \\forall j=1,\\ldots,J\\}.\n",
    "\\end{align}\n",
    "$$\n",
    "and let $x_p$ be the optimal solution of this problem (which we assume to exist for all $\\lambda>0$).\n",
    "\n",
    "In this case, $x_p$ converges to the optimal solution of the problem (if it exists), when $\\lambda \\to 0^+$ (i.e., $\\lambda$ converges to zero from the right)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the good choices barrier algorithm is $\\frac{1}{g_j(x)}$, where $g_j(x)$ is the $j$-th inequation constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def beta(x,f):\n",
    "    _,ieq,_ = f(x)\n",
    "    try:\n",
    "        value = sum([1/max(0, ieq_j) for ieq_j in ieq]) \n",
    "    except ZeroDivisionError:\n",
    "        value = float(\"Inf\")\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def function_with_barrier(x,f,lam):\n",
    "    return f(x)[0]+lam*beta(x,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = minimize(lambda x: function_with_barrier(x,f_constrained,0.00000000000001),\n",
    "         [1,1], method='Nelder-Mead', options={'disp':True})\n",
    "print res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(f_val,ieq,eq) = f_constrained(res.x)\n",
    "print \"Value of f is \"+str(f_val)\n",
    "if len(ieq)>0:\n",
    "    print \"The values of inequality constraints are:\"\n",
    "    for ieq_j in ieq:\n",
    "        print str(ieq_j)+\", \"\n",
    "if len(eq)>0:\n",
    "    print \"The values of the equality constraints are:\"\n",
    "    for eq_k in eq:\n",
    "        print str(eq_k)+\", \"\n",
    "if all([ieq_j>=0 for ieq_j in ieq]) and all([eq_k==0 for eq_k in eq]):\n",
    "    print \"Solution is feasible\"\n",
    "else:\n",
    "    print \"Solution is infeasible\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes about using penalty and barrier function methods\n",
    "\n",
    "* It is worthwile to consider whether feasibility can be compromized. If the constraints do not have any tolerance, then barrier function method should be considered.\n",
    "* Also barrier methods parameter can be set iteratively\n",
    "* Penalty and barrier functions should be chosen so that they are differentiable (thus $x^2$ above)\n",
    "* In both methods, the minimum is attained at the limit.\n",
    "* Different penalty and barrier parameters can be used for differnt constraints, even for same problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Direct methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feasible descent directions\n",
    "\n",
    "Let $S\\subset \\mathbb R^n$ ($S\\neq \\emptyset$ closed 非空闭集) and $x^*\\in S$. \n",
    "\n",
    "**Definition:** The set\n",
    "$$ D = \\{d\\in \\mathbb R^n: d\\neq0,x^*+\\alpha d\\in S \\text{ for all } \\alpha\\in (0,\\delta) \\text{ for some } \\delta>0\\}$$\n",
    "is called the __cone of feasible directions of $S$ in $x^*$ （在$x^*$点处的可行方向锥）__.\n",
    "\n",
    "**Definition:** The set \n",
    "$$ F = \\{d\\in \\mathbb R^n: f(x^*+\\alpha d)<f(x^*)\\text{ for all } \\alpha\\in (0,\\delta) \\text{ for some } \\delta>0\\}$$\n",
    "is called the __cone of descent directions (在$x^*$处的下降方向锥)__.\n",
    "\n",
    "**Definition:** The set $F\\cap D$ is called the __cone of feasible descent directions (可行下降方向锥)__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\S$ Theorem\n",
    "Consider an optimization problem \n",
    "$$\n",
    "\\begin{align}\n",
    "\\min &\\  f(x)\\\\\n",
    "\\text{s.t. }&\\ x\\in S\n",
    "\\end{align}\n",
    "$$\n",
    "and let $x^*\\in S$. Now if $x^*$ is a local minimizer **then** the set of feasible descent directions $F\\cap D$ is empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea for the methods of feasible descent directions\n",
    "\n",
    "1. Assume a feasible solution $x$.\n",
    "2. Find a feasible descent direction $d\\in D\\cap F$.\n",
    "3. Determine the step length to the direction $d$\n",
    "4. Update $x$ accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Rosen's projected gradient method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a problem with __linear equality constraints__:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min\\quad &f(x)\\\\\n",
    "\\text{s.t.}\\quad &Ax=b\n",
    "\\end{align}\n",
    "$$\n",
    "where $A \\in \\mathbb R^{m\\times n}$ ($m\\leq n$) and $b$ is a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x$ be a feasible solution to the above problem.\n",
    "\n",
    "It holds that:\n",
    "\n",
    "> $d$ is a feasible direction *if and only if* $Ad=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the gradient $\\nabla f(x)$ is a feasible descent direction, if \n",
    "$$ A\\nabla f(x)=0.$$\n",
    "\n",
    "This may or may not be true.\n",
    "\n",
    "However, we can project the gradient to the set of feasible descent directions\n",
    "$$ \\{d\\in \\mathbb R^n: Ad=0\\},$$\n",
    "which now is a linear subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection (投影)\n",
    "\n",
    "Let $a\\in \\mathbb R^n$ be a vector and let $L$ be a linear subspace of $\\mathbb R^n$. Now, the following are equivalent\n",
    "* $a^P$ is the projection of $a$ on $L$, ($a^P$是$a$在$L$上的投影)\n",
    "* $\\{a^P\\} = \\operatorname{argmin}_{l\\in L}\\|a-l\\|$, (与$L$上所有向量的距离中，$a$到$a^P$的距离最短)and\n",
    "* $a^P\\in A$ and $(a-a^P)^Tl=0$ for all $l\\in L$ ($a-a^P$与所有$L$上的向量$l$正交)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projected gradient (投影梯度)\n",
    "\n",
    "The projection of the gradient $\\nabla f(x)$ on the set $\\{d\\in \\mathbb R^n: Ad=0\\}$ is denoted by $\\nabla f(x)^P$ and called the __projected gradient__. \n",
    "\n",
    "Now, given some conditions, the projected gradient gives us a feasible descent direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to compute the projected gradient?\n",
    "\n",
    "Basically, the optimization problem that we have to solve is\n",
    "$$\n",
    "\\min\\quad \\|\\nabla f(x)-d\\|\\\\\n",
    "\\text{s.t. }\\quad Ad=0.\n",
    "$$\n",
    "\n",
    "Since it is equivalent to minimize the square of the objective function $\\sum_{i=1}^n \\nabla_i f(x)^2+d_i^2-2\\nabla_i f(x)d_i$, we can see that the problem is a quadratic problem with inequality constraints,\n",
    "$$\n",
    "\\min \\frac12 d^TId-\\nabla f(x)^Td\\\\\n",
    "\\text{s.t. }Ad=0\n",
    "$$\n",
    "which means that we just need to solve the system of equations (see e.g., [Quadratic_programming](https://en.wikipedia.org/wiki/Quadratic_programming#Equality_constraints))\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "I&A^T\\\\\n",
    "A&0\n",
    "\\end{array}\n",
    "\\right] \n",
    "\\left[\\begin{align}d\\\\\\lambda\\end{align}\\right]\n",
    "= \\left[ \n",
    "\\begin{array}{c}\n",
    "\\nabla f(x)\\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right],\n",
    "$$\n",
    "where I is the identity matrix, and $\\lambda$ are the KKT multipliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a function for __projecting a vector to a linear space defined by $Ax=0$__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def project_vector(A,vector):\n",
    "    #convert A into a matrix\n",
    "    A_matrix = np.matrix(A)\n",
    "    #construct the \"first row\" of the matrix [[I,A^T],[A,0]]\n",
    "    left_matrix_first_row = np.concatenate((np.identity(len(vector)),\\\n",
    "                                            A_matrix.transpose()), axis=1)\n",
    "    #construct the \"second row\" of the matrix\n",
    "    left_matrix_second_row = np.concatenate((A_matrix,np.matrix(np.zeros([len(A),\\\n",
    "                                            len(vector)+len(A)-len(A[0])]))), axis=1)\n",
    "    #combine the whole matrix by combining the rows\n",
    "    left_matrix = np.concatenate((left_matrix_first_row,left_matrix_second_row),axis = 0)\n",
    "    #Solve the system of linear equalities from the previous page\n",
    "    return np.linalg.solve(left_matrix, \\\n",
    "                           np.concatenate((np.matrix(vector).transpose(),\\\n",
    "                                           np.zeros([len(A),1])),axis=0))[:len(vector)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = [[1,0,0],[0,1,0]]\n",
    "gradient = [1,1,1]\n",
    "project_vector(A,gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "Say we study an optimization problem\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\qquad& x_1^2+x_2^2+x_3^2\\\\\n",
    "\\text{s.t.}\\qquad &x_1+x_2=3\\\\\n",
    "    &x_1+x_3=4.\n",
    "\\end{align}\n",
    "$$\n",
    "Let us project a gradient from a feasible point $x=(1,2,3)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ad\n",
    "A = [[1,1,0],[1,0,1]]\n",
    "gradient = ad.gh(lambda x:x[0]**2+x[1]**2+x[2]**2)[0]([1,2,3])\n",
    "d = project_vector(A,[-i for i in gradient])\n",
    "print d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Here $d$ is a feasible direction since__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.matrix(A)*d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__$d$ is also a descent direction since__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x[0]**2+x[1]**2+x[2]**2\n",
    "alpha = 0.001\n",
    "print \"Value of f at [1,2,3] is \"+str(f([1,2,3]))\n",
    "x_mod= np.array([1,2,3])+alpha*np.array(d).transpose()[0]\n",
    "print \"Value of f at [1,2,3] + alpha*d is \"+str(f(x_mod))\n",
    "print \"Gradient dot product direction (i.e., directional derivative) is \" \\\n",
    "+ str(np.matrix(ad.gh(f)[0]([1,2,3])).dot(np.array(d)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try projected gradient descent method to solve the optimization problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ad\n",
    "def projected_gradient_method(f,A,start,step,precision):\n",
    "    f_old = float('Inf')\n",
    "    x = np.array(start)\n",
    "    steps = []\n",
    "    f_new = f(x)\n",
    "    while abs(f_old-f_new)>precision:\n",
    "        f_old = f_new\n",
    "        gradient = ad.gh(f)[0](x)\n",
    "        grad_proj = project_vector(A,[-i for i in gradient])#The only changes to steepest..\n",
    "        grad_proj = np.array(grad_proj.transpose())[0] #... descent are here!\n",
    "#        import pdb; pdb.set_trace()\n",
    "        x = x+grad_proj*step\n",
    "        f_new = f(x)\n",
    "        steps.append(list(x))\n",
    "    return x,f_new,steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = lambda x:x[0]**2+x[1]**2+x[2]**2\n",
    "A = [[1,1,0],[1,0,1]]\n",
    "start = [1,2,3]\n",
    "(x,f_val,steps) = projected_gradient_method(f,A,start,0.6,0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, check the result to see whether the objective value is smaller and if the solution is feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print x\n",
    "print f(x)\n",
    "print f([1,2,3])\n",
    "print np.matrix(A)*(np.matrix(x).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
